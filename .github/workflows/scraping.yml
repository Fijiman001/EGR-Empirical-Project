name: Scrape Green Bond Data

on:
  schedule:
    # Runs at 0 minutes past the hour, every hour, Monday, Wednesday, and Friday
    - cron: '0 8-18 * * 1-5'  # Modify this as per your desired schedule

  workflow_dispatch:  # Allows you to manually trigger this workflow from GitHub UI

jobs:
  scrape:
    runs-on: ubuntu-22.04  # Use the latest version of Ubuntu for the environment

    steps:
      # Check out the repository
      - name: Check out the repository
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python 3.x
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'  # Use Python 3.x version

      # Install necessary packages
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4

      - name: Install Chrome and ChromeDriver
        run: |
          sudo apt update
          sudo apt install -y google-chrome-stable
          sudo apt install -y chromium-browser chromium-chromedriver
          ln -sf /snap/bin/chromium /usr/bin/google-chrome  # Symlink snap binary to standard location
          ln -sf /usr/bin/chromedriver /usr/local/bin/chromedriver  # Ensure ChromeDriver is accessible
          which google-chrome
          which chromedriver
          ls -l /usr/bin/google-chrome
          ls -l /usr/local/bin/chromedriver

      # Run the scraping script
      - name: Run scraping script
        env:
          DISPLAY: ":99"  # Set DISPLAY for headless Chromium
        run: |
          python code/webscraping_bond_data_automated.py  # Replace with the name of your script

      # Upload saved files to repo
      - name: Upload generated CSV files
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data
          path: |
            green_bonds_data_*.csv
            green_bonds_partial_data_*.csv
            all_bonds_data_*.csv
            all_bonds_partial_data_*.csv
