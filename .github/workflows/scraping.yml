name: Scrape Green Bond Data

on:
  schedule:
    - cron: '0 8-18 * * 1-5'  # Runs every hour during business hours, Monday to Friday
  workflow_dispatch:  # Allows manual triggering from GitHub UI

jobs:
  scraping:
    runs-on: ubuntu-latest

    steps:
      # Check out the repository
      - name: Check out the repository
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'  # Specify a Python version

      # Install necessary packages
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip
          sudo apt-get install -y xvfb
          wget -N https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip -d /usr/local/bin/
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4

      # Set up display for headless Chrome
      - name: Start Xvfb
        run: |
          export DISPLAY=:99.0
          Xvfb :99 -screen 0 1920x1080x16 &

      # Run the scraping script
      - name: Run scraping script
        env:
          DISPLAY: ":99"  # Needed for headless Chrome
        run: |
          python code/webscraping_bond_data_automated.py

      # Upload generated CSV files
      - name: Upload CSV files
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data
          path: |
            green_bonds_data_*.csv
            green_bonds_partial_data_*.csv
            all_bonds_data_*.csv
            all_bonds_partial_data_*.csv
