name: Scrape Green Bond Data

on:
  schedule:
    - cron: '0 8-18 * * 1-5'  # Runs every hour during business hours, Monday to Friday
  workflow_dispatch:  # Allows manual triggering from GitHub UI

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # Check out the repository
      - name: Check out the repository
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'  # Specify a Python version

      # Install necessary packages
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4

      # Install Google Chrome and ChromeDriver
      - name: Install Chrome and ChromeDriver
        run: |
          # Download and install the latest stable Google Chrome
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

          # Download ChromeDriver matching the installed Chrome version
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
          wget -N https://chromedriver.storage.googleapis.com/$CHROME_VERSION.0.5481.77/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver

          # Verify installations
          google-chrome --version
          chromedriver --version

      # Run the scraping script
      - name: Run scraping script
        env:
          DISPLAY: ":99"  # Needed for headless Chrome
        run: |
          python code/webscraping_bond_data_automated.py

      # Upload generated CSV files
      - name: Upload CSV files
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data
          path: |
            green_bonds_data_*.csv
            green_bonds_partial_data_*.csv
            all_bonds_data_*.csv
            all_bonds_partial_data_*.csv
