name: Scrape Green Bond Data

on:
  schedule:
    - cron: '0 8-16 * * 1-5'  # Runs every hour during business hours, Monday to Friday
  workflow_dispatch:  # Allows manual triggering from GitHub UI

jobs:
  scraping:
    runs-on: ubuntu-latest

    steps:
      # Check out the repository
      - name: Check out the repository
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'  # Specify a Python version

      # Install necessary packages
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip
          sudo apt-get install -y xvfb
          wget -N https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip -d /usr/local/bin/
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4

      # Set up display for headless Chrome
      - name: Start Xvfb
        run: |
          export DISPLAY=:99.0
          Xvfb :99 -screen 0 1920x1080x16 &

      # Run the scraping script
      - name: Run scraping script
        env:
          DISPLAY: ":99"  # Needed for headless Chrome
        run: |
          python code/webscraping_bond_data_automated.py

      # List generated files for debugging
      - name: List generated CSV files
        run: ls -lah *.csv

      # Get the current date
      - name: Get current date
        id: date
        run: echo "DATE=$(date +'%Y-%m-%d')" >> $GITHUB_ENV

      # Upload generated CSV files
      - name: Upload CSV files
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data-${{ env.DATE }}
          path: |
            green_bonds_data_*.csv
            green_bonds_partial_data_*.csv
            all_bonds_data_*.csv
            all_bonds_partial_data_*.csv

      # Push artifacts to the data directory in the repository
      - name: Push files to repository
        if: success()
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git checkout main
          mkdir -p data
          cp green_bonds_data_*.csv data/ || true
          cp green_bonds_partial_data_*.csv data/ || true
          cp all_bonds_data_*.csv data/ || true
          cp all_bonds_partial_data_*.csv data/ || true
          git add data/
          git commit -m "Add scraped data from ${{ env.DATE }}"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
