{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to implement and test different matching methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why we did matching? plot average distances of matching. Think of a KPI. Illustrate with one graph or visualisation, this can be a good distance measure, then back up with literature, the ECB article.\n",
    "\n",
    "Comapre the coverage, do we have more bonds than taking a very strict matching algorithm, Create a table with different matching algorithms and the results. Show that we used different matching methods.\n",
    "\n",
    "emphasise that the data is scarce, we do not want to lose too much data, get better results with broader scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'Clean_Company_Maturity', 'WKN', 'URL', 'ISIN', 'Company',\n",
      "       'Kupon_Maturity', 'Coupon', 'Maturity_Start', 'Maturity_End',\n",
      "       'is_green', 'Last Price', 'DateTime', 'Volume in Euro', '+/- %',\n",
      "       'Currency', 'YTM', 'Modified Duration', 'Kupon', 'Emittent', 'Branche',\n",
      "       'Fälligkeit', 'Schuldnerkündigungsart', 'Sonderkündigung', 'Nachrangig',\n",
      "       'Kleinste handelbare Einheit', 'Spezialist', 'Handelsmodell',\n",
      "       'Emissionsdatum', 'Emissionsvolumen', 'Umlaufendes Volumen',\n",
      "       'Emissionswährung', 'Depotwährung', 'Notierungsaufnahme',\n",
      "       'Emission_year_2020.0', 'Emission_year_2021.0', 'Emission_year_2022.0',\n",
      "       'Emission_year_2023.0', 'Emission_year_2024.0', 'Emission_year_2025.0',\n",
      "       'Date', 'fälligkeit_datum', 'emission_datum', 'days_to_maturity'],\n",
      "      dtype='object')\n",
      "                   Name Clean_Company_Maturity     WKN  \\\n",
      "0  E.ON SE 0,375% 20/27    e on se 0 375 20 27  A254QR   \n",
      "1  E.ON SE 0,375% 20/27    e on se 0 375 20 27  A254QR   \n",
      "2  E.ON SE 0,375% 20/27    e on se 0 375 20 27  A254QR   \n",
      "3  E.ON SE 0,375% 20/27    e on se 0 375 20 27  A254QR   \n",
      "4  E.ON SE 0,375% 20/27    e on se 0 375 20 27  A254QR   \n",
      "\n",
      "                                                 URL          ISIN  Company  \\\n",
      "0  https://www.boerse-frankfurt.de/anleihe/xs2103...  xs2103014291  e on se   \n",
      "1  https://www.boerse-frankfurt.de/anleihe/xs2103...  xs2103014291  e on se   \n",
      "2  https://www.boerse-frankfurt.de/anleihe/xs2103...  xs2103014291  e on se   \n",
      "3  https://www.boerse-frankfurt.de/anleihe/xs2103...  xs2103014291  e on se   \n",
      "4  https://www.boerse-frankfurt.de/anleihe/xs2103...  xs2103014291  e on se   \n",
      "\n",
      "  Kupon_Maturity  Coupon  Maturity_Start  Maturity_End  ...  \\\n",
      "0    0 375 20 27   0.375              20            27  ...   \n",
      "1    0 375 20 27   0.375              20            27  ...   \n",
      "2    0 375 20 27   0.375              20            27  ...   \n",
      "3    0 375 20 27   0.375              20            27  ...   \n",
      "4    0 375 20 27   0.375              20            27  ...   \n",
      "\n",
      "   Emission_year_2020.0  Emission_year_2021.0 Emission_year_2022.0  \\\n",
      "0                  True                 False                False   \n",
      "1                  True                 False                False   \n",
      "2                  True                 False                False   \n",
      "3                  True                 False                False   \n",
      "4                  True                 False                False   \n",
      "\n",
      "   Emission_year_2023.0 Emission_year_2024.0 Emission_year_2025.0        Date  \\\n",
      "0                 False                False                False  2025-01-02   \n",
      "1                 False                False                False  2025-01-02   \n",
      "2                 False                False                False  2025-01-02   \n",
      "3                 False                False                False  2025-01-03   \n",
      "4                 False                False                False  2025-01-03   \n",
      "\n",
      "  fälligkeit_datum emission_datum days_to_maturity  \n",
      "0       2027-09-29     2020-01-16           1000.0  \n",
      "1       2027-09-29     2020-01-16           1000.0  \n",
      "2       2027-09-29     2020-01-16           1000.0  \n",
      "3       2027-09-29     2020-01-16            999.0  \n",
      "4       2027-09-29     2020-01-16            999.0  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Fijiman001/EGR-Empirical-Project/refs/heads/main/data/Final%20data%20for%20the%20study/df_final_7.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m euclidean_distances\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch_green_bonds\u001b[39m(df):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Ensure the dataframe has the necessary columns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def match_green_bonds(df):\n",
    "    # Ensure the dataframe has the necessary columns\n",
    "    required_cols = ['Company', 'DateTime', 'Coupon', 'days_to_maturity', 'Emission_year_2020.0', \n",
    "                    'Emission_year_2021.0', 'Emission_year_2022.0', \n",
    "                    'Emission_year_2023.0', 'Emission_year_2024.0', \n",
    "                    'Emission_year_2025.0', 'is_green', 'SchuldnerkÃ¼ndigungsart']\n",
    "    \n",
    "    # Add is_green column if it doesn't exist (for testing)\n",
    "    if 'is_green' not in df.columns:\n",
    "        # This is just for testing - you would replace this with your actual data\n",
    "        df['is_green'] = np.random.choice([0, 1], size=len(df))\n",
    "    \n",
    "    # Create features for matching\n",
    "    features = ['Coupon', 'days_to_maturity']\n",
    "    \n",
    "    # Add emission year as a feature\n",
    "    emission_cols = [col for col in df.columns if col.startswith('Emission_year_')]\n",
    "    \n",
    "    # Create a function to convert emission year columns to a single value\n",
    "    def get_emission_year(row):\n",
    "        for col in emission_cols:\n",
    "            if row[col]:\n",
    "                # Extract year from column name (e.g., 'Emission_year_2020.0' -> 2020)\n",
    "                return float(col.split('_')[2].split('.')[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['emission_year'] = df.apply(get_emission_year, axis=1)\n",
    "    features.append('emission_year')\n",
    "    \n",
    "    # Filter out rows with missing values in crucial columns\n",
    "    df_clean = df.dropna(subset=features + ['Company', 'is_green'])\n",
    "    \n",
    "    # Get unique companies\n",
    "    companies = df_clean['Company'].unique()\n",
    "    \n",
    "    # Prepare results dataframe\n",
    "    results = []\n",
    "    \n",
    "    # For each company, match green bonds with conventional bonds\n",
    "    for company in companies:\n",
    "        company_bonds = df_clean[df_clean['Company'] == company]\n",
    "        \n",
    "        # Skip if there are no green bonds or no conventional bonds\n",
    "        if 1 not in company_bonds['is_green'].values or 0 not in company_bonds['is_green'].values:\n",
    "            continue\n",
    "        \n",
    "        green_bonds = company_bonds[company_bonds['is_green'] == 1]\n",
    "        conv_bonds = company_bonds[company_bonds['is_green'] == 0]\n",
    "        \n",
    "        # Extract features for matching\n",
    "        green_features = green_bonds[features].values\n",
    "        conv_features = conv_bonds[features].values\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        green_scaled = scaler.fit_transform(green_features)\n",
    "        conv_scaled = scaler.transform(conv_features)\n",
    "        \n",
    "        # Calculate distances between each green bond and each conventional bond\n",
    "        distances = euclidean_distances(green_scaled, conv_scaled)\n",
    "        \n",
    "        # Match each green bond with the closest conventional bond\n",
    "        for i, green_idx in enumerate(green_bonds.index):\n",
    "            closest_conv_idx = conv_bonds.index[np.argmin(distances[i])]\n",
    "            \n",
    "            # Create a pair\n",
    "            green_bond = green_bonds.loc[green_idx]\n",
    "            conv_bond = conv_bonds.loc[closest_conv_idx]\n",
    "            \n",
    "            results.append({\n",
    "                'Company': company,\n",
    "                'Green_Bond_Name': green_bond['Name'] if 'Name' in green_bond.index else 'Unknown',\n",
    "                'Green_Bond_ISIN': green_bond['ISIN'] if 'ISIN' in green_bond.index else 'Unknown',\n",
    "                'Green_Bond_Coupon': green_bond['Coupon'],\n",
    "                'Green_Bond_Maturity': green_bond['days_to_maturity'],\n",
    "                'Green_Bond_Emission_Year': green_bond['emission_year'],\n",
    "                'Conv_Bond_Name': conv_bond['Name'] if 'Name' in conv_bond.index else 'Unknown',\n",
    "                'Conv_Bond_ISIN': conv_bond['ISIN'] if 'ISIN' in conv_bond.index else 'Unknown',\n",
    "                'Conv_Bond_Coupon': conv_bond['Coupon'],\n",
    "                'Conv_Bond_Maturity': conv_bond['days_to_maturity'],\n",
    "                'Conv_Bond_Emission_Year': conv_bond['emission_year'],\n",
    "                'Distance': distances[i][np.argmin(distances[i])]\n",
    "            })\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "matched_bonds = match_green_bonds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for any given green bond, take the match which has the smallest distance\n",
    "take average distance. group by bonds and conventional bond unique name, compute mean average distance.\n",
    "then take smallest pair in average.\n",
    "\n",
    "weight the coupon in the distance measure a bit less, put more weight on the days to maturity.\n",
    "include emission date measure. \n",
    "\n",
    "Show this in our analysis.\n",
    "\n",
    "test YTM vs. Z-spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(matched_bonds['Green_Bond_ISIN'].nunique()) 100 unique green bond ISINs\n",
    "# dropping duplicates\n",
    "matched_bonds.drop_duplicates(inplace=True)\n",
    "# no longer need other columns other than the ISINS\n",
    "matched_bonds_ISIN = matched_bonds[['Green_Bond_ISIN', 'Conv_Bond_ISIN', 'Distance']]\n",
    "matched_bonds_ISIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using this matching method, we get 100 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_regression_data(matched_bonds, original_df):\n",
    "    \"\"\"\n",
    "    Prepare the regression dataset from matched green and conventional bonds.\n",
    "    \n",
    "    Parameters:\n",
    "    matched_bonds (DataFrame): The matched pairs of green and conventional bonds\n",
    "    original_df (DataFrame): The original dataset with YTM information\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Clean dataset ready for regression analysis\n",
    "    \"\"\"\n",
    "    # Create regression dataset from matched pairs\n",
    "    regression_data = []\n",
    "    \n",
    "    for _, row in matched_bonds.iterrows():\n",
    "        # Green bond data\n",
    "        green_data = {\n",
    "            'ISIN': row['Green_Bond_ISIN'],\n",
    "            'Company': row['Company'],\n",
    "            'Coupon': row['Green_Bond_Coupon'],\n",
    "            'Maturity': row['Green_Bond_Maturity'],\n",
    "            'Emission_Year': row['Green_Bond_Emission_Year'],\n",
    "            'is_green': 1,\n",
    "            'Distance': row['Distance'],\n",
    "            'Bond_Pair_ID': _  # Use row index as pair identifier\n",
    "        }\n",
    "        \n",
    "        # Conventional bond data\n",
    "        conv_data = {\n",
    "            'ISIN': row['Conv_Bond_ISIN'],\n",
    "            'Company': row['Company'],\n",
    "            'Coupon': row['Conv_Bond_Coupon'],\n",
    "            'Maturity': row['Conv_Bond_Maturity'],\n",
    "            'Emission_Year': row['Conv_Bond_Emission_Year'],\n",
    "            'is_green': 0,\n",
    "            'Distance': row['Distance'],\n",
    "            'Bond_Pair_ID': _  # Same pair identifier\n",
    "        }\n",
    "        \n",
    "        regression_data.append(green_data)\n",
    "        regression_data.append(conv_data)\n",
    "    \n",
    "    # Create dataframe\n",
    "    reg_df = pd.DataFrame(regression_data)\n",
    "    \n",
    "    # Merge with original data to get YTM and DateTime\n",
    "    reg_df = pd.merge(\n",
    "        reg_df,\n",
    "        original_df[['ISIN', 'YTM', 'DateTime']],\n",
    "        on='ISIN',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Convert all numeric columns to float\n",
    "    numeric_cols = ['is_green', 'Coupon', 'Maturity', 'Distance', 'YTM', 'Emission_Year']\n",
    "    for col in numeric_cols:\n",
    "        reg_df[col] = pd.to_numeric(reg_df[col], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing values in key columns\n",
    "    reg_df = reg_df.dropna(subset=['YTM', 'is_green', 'Coupon', 'Maturity', 'Distance'])\n",
    "    \n",
    "    # Handle company dummies properly\n",
    "    # First ensure Company column has proper encoding and clean values\n",
    "    reg_df['Company'] = reg_df['Company'].astype(str).str.strip()\n",
    "    \n",
    "    # Let's print the unique companies to verify\n",
    "    print(f\"Number of unique companies: {reg_df['Company'].nunique()}\")\n",
    "    print(f\"Company values: {reg_df['Company'].unique()[:5]}...\")  # Show first 5\n",
    "    \n",
    "    # Create company dummy variables - ensure they're properly encoded\n",
    "    company_cols = pd.get_dummies(reg_df['Company'], prefix='Company', drop_first=True)\n",
    "    \n",
    "    # Instead of using get_dummies on the entire dataframe, combine manually\n",
    "    reg_df = pd.concat([reg_df, company_cols], axis=1)\n",
    "    \n",
    "    return reg_df\n",
    "\n",
    "df_regression = prepare_regression_data(matched_bonds, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First prepare data\n",
    "reg_df = prepare_regression_data(matched_bonds, df)\n",
    "\n",
    "# For regression analysis, select appropriate columns\n",
    "X_cols = ['is_green', 'Coupon', 'Maturity', 'Distance']\n",
    "company_cols = [col for col in reg_df.columns if col.startswith('Company_')]\n",
    "\n",
    "# Combine main variables with company dummies\n",
    "X = reg_df[X_cols + company_cols]\n",
    "y = reg_df['YTM']\n",
    "\n",
    "# Add constant and run regression\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
