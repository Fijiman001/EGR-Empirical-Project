{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OvpTU4tTfWxR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages & Function to Get Data"
      ],
      "metadata": {
        "id": "eWDjrXzCHb3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import time\n",
        "import shutil"
      ],
      "metadata": {
        "id": "f010x_UGFH6G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract bond data from the current page\n",
        "def extract_bond_data(soup):\n",
        "    bond_table = soup.find(\"div\", {\"class\": \"table-responsive\"})\n",
        "    bonds = []\n",
        "\n",
        "    if bond_table:\n",
        "        rows = bond_table.find_all(\"tr\")  # Get all rows in the table\n",
        "\n",
        "        # Iterate over each row (skip the header row)\n",
        "        for row in rows[1:]:\n",
        "            cols = row.find_all(\"td\")  # Get all columns in the row\n",
        "            if len(cols) >= 6:  # Ensure there are enough columns\n",
        "                bond_info = {\n",
        "                    \"Name\": cols[0].text.strip(),\n",
        "                    \"WKN\": cols[1].text.strip(),\n",
        "                    \"Last Price\": cols[2].text.strip(),\n",
        "                    \"Date/Time Last Price\": cols[3].text.strip(),\n",
        "                    \"Volume in Euro\": cols[4].text.strip(),\n",
        "                    \"+/- %\": cols[5].text.strip(),\n",
        "                    \"Coupon\": cols[12].text.strip(),\n",
        "                    \"Currency\": cols[13].text.strip(),\n",
        "                    \"YTM\": cols[14].text.strip()\n",
        "                }\n",
        "                bonds.append(bond_info)\n",
        "    return bonds"
      ],
      "metadata": {
        "id": "BSBSpUU0Habc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Green bond data"
      ],
      "metadata": {
        "id": "rA9ZOKtRbStl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if Chromedriver is available on system path\n",
        "shutil.which(\"chromedriver\")\n",
        "\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialize the Selenium WebDriver with the options\n",
        "driver = webdriver.Chrome(options=chrome_options)  # Or use webdriver.Firefox(), etc.\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://www.boerse-frankfurt.de/anleihen/green-bonds\"\n",
        "\n",
        "try:\n",
        "    # Open the website\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the \"100\" button to be visible and clickable\n",
        "    wait = WebDriverWait(driver, 5)\n",
        "    hundred_button = wait.until(\n",
        "        EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'page-bar-type-button btn btn-lg ng-star-inserted') and text()='100']\"))\n",
        "    )\n",
        "\n",
        "    # Click the \"100\" button\n",
        "    hundred_button.click()\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Initialize an empty list to store all bonds\n",
        "    all_bonds = []\n",
        "\n",
        "    # Find the number of pages\n",
        "    page_buttons = driver.find_elements(By.XPATH, \"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and not(@disabled)]\")\n",
        "    total_pages = int(page_buttons[-1].text.strip())  # Extract the number from the last button, this button should always be showing the maximum amount of pages\n",
        "    print(f\"Total pages: {total_pages}\")\n",
        "\n",
        "    # Loop through all pages, skipping the first one\n",
        "    for page in range(1, total_pages + 1):\n",
        "      try:\n",
        "\n",
        "        # Wait for the page button to be clickable and click it\n",
        "        if page != 1:\n",
        "          page_button = wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH, f\"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and text()='{page}']\"))\n",
        "          )\n",
        "          page_button.click()\n",
        "          time.sleep(5)  # Allow time for the page to load, check how long is optimal, 2 seconds to be safe\n",
        "\n",
        "        # Extract page source and parse with BeautifulSoup\n",
        "        page_source = driver.page_source\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "        # Extract data from the current page\n",
        "        bonds = extract_bond_data(soup)\n",
        "        all_bonds.extend(bonds)\n",
        "\n",
        "        # Loop count\n",
        "        print(f\"Loop {page}\")\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"An error occurred on page {page}: {e}\")\n",
        "        break  # Exit the loop if there is an error\n",
        "\n",
        "    # Get the current date and time for the file name\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    file_name = f\"green_bonds_data_{timestamp}.csv\"\n",
        "\n",
        "    # Save all the extracted data to a CSV file\n",
        "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"Name\", \"WKN\", \"Last Price\", \"Date/Time Last Price\", \"Volume in Euro\", \"+/- %\", \"Coupon\", \"Currency\", \"YTM\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()  # Write the header row\n",
        "        writer.writerows(all_bonds)  # Write all bond data rows\n",
        "\n",
        "    print(\"Bond data has been saved to 'Green_bonds_data.csv'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Critical error: {e}\")\n",
        "\n",
        "    # Get the current date and time for the file name\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    file_name = f\"green_bonds_partial_data_{timestamp}.csv\"\n",
        "\n",
        "    # Save the collected data so far\n",
        "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"Name\", \"WKN\", \"Last Price\", \"Date/Time Last Price\", \"Volume in Euro\", \"+/- %\", \"Coupon\", \"Currency\", \"YTM\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()  # Write the header row\n",
        "        writer.writerows(all_bonds)  # Write collected bond data rows so far\n",
        "\n",
        "    print(\"Partial bond data has been saved to 'Green_bonds_partial_data.csv'\")\n",
        "\n",
        "finally:\n",
        "    # Close the browser\n",
        "    driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vRO3PZ8i1gK",
        "outputId": "ca3b8fce-6bc8-491f-8513-4bc04a6c199e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 5\n",
            "Loop 1\n",
            "Loop 2\n",
            "Loop 3\n",
            "Loop 4\n",
            "Loop 5\n",
            "Bond data has been saved to 'Green_bonds_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All Bonds"
      ],
      "metadata": {
        "id": "4P-o_jYYyX-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHXoY5ngbD_O",
        "outputId": "a20a5513-540f-4bd4-9dc1-41153b3ed3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 354\n",
            "Loop 1\n",
            "Loop 2\n",
            "Loop 3\n",
            "Loop 4\n",
            "Loop 5\n",
            "Loop 6\n",
            "Loop 7\n",
            "Loop 8\n",
            "Loop 9\n",
            "Loop 10\n",
            "Loop 11\n",
            "Loop 12\n",
            "Loop 13\n",
            "Loop 14\n",
            "Loop 15\n",
            "Loop 16\n",
            "Loop 17\n",
            "Loop 18\n",
            "Loop 19\n",
            "Loop 20\n",
            "Bond data has been saved to 'all_bonds_data.csv'\n"
          ]
        }
      ],
      "source": [
        "# Check if Chromedriver is available on system path\n",
        "shutil.which(\"chromedriver\")\n",
        "\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialize the Selenium WebDriver with the options\n",
        "driver = webdriver.Chrome(options=chrome_options)  # Or use webdriver.Firefox(), etc.\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://www.boerse-frankfurt.de/anleihen/most-traded\"\n",
        "\n",
        "try:\n",
        "    # Open the website\n",
        "    driver.get(url)\n",
        "\n",
        "    # Check if needed\n",
        "    time.sleep(5)\n",
        "\n",
        "    # <button _ngcontent-boerse-frankfurt-c115\n",
        "    # Wait for the \"100\" button to be visible and clickable\n",
        "    wait = WebDriverWait(driver, 1)\n",
        "    hundred_button = wait.until(\n",
        "        EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'page-bar-type-button btn btn-lg ng-star-inserted') and text()='100']\"))\n",
        "    )\n",
        "    # Click the \"100\" button\n",
        "    hundred_button.click()\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Initialize an empty list to store all bonds\n",
        "    all_bonds = []\n",
        "\n",
        "    # Find the number of pages\n",
        "    page_buttons = driver.find_elements(By.XPATH, \"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and not(@disabled)]\")\n",
        "    total_pages = int(page_buttons[-1].text.strip())  # Extract the number from the last button, this button should always be showing the maximum amount of pages\n",
        "    print(f\"Total pages: {total_pages}\")\n",
        "\n",
        "    # Loop through all pages, skipping the first one, limiting to 20 pages, so the 2000 most traded bonds\n",
        "    for page in range(1, min(total_pages + 1, 21)):\n",
        "      try:\n",
        "\n",
        "        # Wait for the page button to be clickable and click it\n",
        "        if page != 1:\n",
        "          page_button = wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH, f\"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and text()='{page}']\"))\n",
        "          )\n",
        "          page_button.click()\n",
        "          time.sleep(5)  # Allow time for the page to load, check how long is optimal\n",
        "\n",
        "        # Extract page source and parse with BeautifulSoup\n",
        "        page_source = driver.page_source\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "        # Extract data from the current page\n",
        "        bonds = extract_bond_data(soup)\n",
        "        all_bonds.extend(bonds)\n",
        "\n",
        "        # Loop count\n",
        "        print(f\"Loop {page}\")\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"An error occurred on page {page}: {e}\")\n",
        "        break  # Exit the loop if there is an error\n",
        "\n",
        "    # Get the current date and time for the file name\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    file_name = f\"All_bonds_data_{timestamp}.csv\"\n",
        "\n",
        "    # Save all the extracted data to a CSV file\n",
        "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"Name\", \"WKN\", \"Last Price\", \"Date/Time Last Price\", \"Volume in Euro\", \"+/- %\", \"Coupon\", \"Currency\", \"YTM\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()  # Write the header row\n",
        "        writer.writerows(all_bonds)  # Write all bond data rows\n",
        "\n",
        "    print(\"Bond data has been saved to 'all_bonds_data.csv'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Critical error: {e}\")\n",
        "\n",
        "    # Get the current date and time for the file name\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    file_name = f\"All_bonds_partial_data_{timestamp}.csv\"\n",
        "\n",
        "    # Save the collected data so far\n",
        "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"Name\", \"WKN\", \"Last Price\", \"Date/Time Last Price\", \"Volume in Euro\", \"+/- %\", \"Coupon\", \"Currency\", \"YTM\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()  # Write the header row\n",
        "        writer.writerows(all_bonds)  # Write collected bond data rows so far\n",
        "\n",
        "    print(\"Partial bond data has been saved to 'all_bonds_partial_data.csv'\")\n",
        "\n",
        "finally:\n",
        "    # Close the browser\n",
        "    driver.quit()"
      ]
    }
  ]
}