{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgB8vYE+7ddHpmSLN9TKHF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fijiman001/EGR-Empirical-Project/blob/main/Webscraping_Bond_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OvpTU4tTfWxR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Green bond data"
      ],
      "metadata": {
        "id": "rA9ZOKtRbStl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import shutil\n",
        "shutil.which(\"chromedriver\")\n",
        "\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialize the Selenium WebDriver with the options\n",
        "driver = webdriver.Chrome(options=chrome_options)  # Or use webdriver.Firefox(), etc.\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://www.boerse-frankfurt.de/anleihen/green-bonds\"\n",
        "\n",
        "# Function to extract bond data from the current page\n",
        "def extract_bond_data(soup):\n",
        "    bond_table = soup.find(\"div\", {\"class\": \"table-responsive\"})\n",
        "    bonds = []\n",
        "\n",
        "    if bond_table:\n",
        "        rows = bond_table.find_all(\"tr\")  # Get all rows in the table\n",
        "\n",
        "        # Iterate over each row (skip the header row)\n",
        "        for row in rows[1:]:\n",
        "            cols = row.find_all(\"td\")  # Get all columns in the row\n",
        "            if len(cols) >= 6:  # Ensure there are enough columns\n",
        "                bond_info = {\n",
        "                    \"Name\": cols[0].text.strip(),\n",
        "                    \"WKN\": cols[1].text.strip(),\n",
        "                    \"Letzter Preis\": cols[2].text.strip(),\n",
        "                    \"Datum/Zeit letzter Preis\": cols[3].text.strip(),\n",
        "                    \"Umsatz in Euro\": cols[4].text.strip(),\n",
        "                    \"+/- %\": cols[5].text.strip(),\n",
        "                    \"Coupon\": cols[12].text.strip(),\n",
        "                    \"Currency\": cols[13].text.strip(),\n",
        "                    \"YTM\": cols[14].text.strip()\n",
        "                }\n",
        "                bonds.append(bond_info)\n",
        "    return bonds\n",
        "\n",
        "try:\n",
        "    # Open the website\n",
        "    driver.get(url)\n",
        "\n",
        "    # Check if needed\n",
        "    time.sleep(2)\n",
        "\n",
        "    # <button _ngcontent-boerse-frankfurt-c115\n",
        "    # Wait for the \"100\" button to be visible and clickable\n",
        "    wait = WebDriverWait(driver, 1)\n",
        "    hundred_button = wait.until(\n",
        "        EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'page-bar-type-button btn btn-lg ng-star-inserted') and text()='100']\"))\n",
        "    )\n",
        "\n",
        "    # Click the \"100\" button\n",
        "    hundred_button.click()\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Initialize an empty list to store all bonds\n",
        "    all_bonds = []\n",
        "\n",
        "    # Find the number of pages\n",
        "    page_buttons = driver.find_elements(By.XPATH, \"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and not(@disabled)]\")\n",
        "    total_pages = int(page_buttons[-1].text.strip())  # Extract the number from the last button, this button should always be showing the maximum amount of pages\n",
        "    print(f\"Total pages: {total_pages}\")\n",
        "\n",
        "    # Loop through all pages, skipping the first one\n",
        "    for page in range(1, total_pages + 1):\n",
        "\n",
        "        # Wait for the page button to be clickable and click it\n",
        "        if page != 1:\n",
        "          page_button = wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH, f\"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and text()='{page}']\"))\n",
        "          )\n",
        "          page_button.click()\n",
        "          time.sleep(2)  # Allow time for the page to load, check how long is optimal, 2 seconds to be safe\n",
        "\n",
        "        # Extract page source and parse with BeautifulSoup\n",
        "        page_source = driver.page_source\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "        # Extract data from the current page\n",
        "        bonds = extract_bond_data(soup)\n",
        "        all_bonds.extend(bonds)\n",
        "\n",
        "        # Loop count\n",
        "        print(f\"Loop {page}\")\n",
        "\n",
        "    # Save all the extracted data to a CSV file\n",
        "    with open(\"Green_bonds_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"Name\", \"WKN\", \"Letzter Preis\", \"Datum/Zeit letzter Preis\", \"Umsatz in Euro\", \"+/- %\", \"Coupon\", \"Currency\", \"YTM\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()  # Write the header row\n",
        "        writer.writerows(all_bonds)  # Write all bond data rows\n",
        "\n",
        "    print(\"Bond data has been saved to 'Green_bonds_data.csv'\")\n",
        "\n",
        "finally:\n",
        "    # Close the browser\n",
        "    driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vRO3PZ8i1gK",
        "outputId": "cbb014cc-d4f2-4530-80dd-226842252365"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 5\n",
            "Loop 1\n",
            "Loop 2\n",
            "Loop 3\n",
            "Loop 4\n",
            "Loop 5\n",
            "Bond data has been saved to 'Green_bonds_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All Bonds"
      ],
      "metadata": {
        "id": "4P-o_jYYyX-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHXoY5ngbD_O",
        "outputId": "da6261ed-9978-4489-9924-097cebf2df3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 354\n",
            "Loop 1\n",
            "Loop 2\n",
            "Loop 3\n",
            "Loop 4\n",
            "Bond data has been saved to 'all_bonds_data.csv'\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import shutil\n",
        "shutil.which(\"chromedriver\")\n",
        "\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialize the Selenium WebDriver with the options\n",
        "driver = webdriver.Chrome(options=chrome_options)  # Or use webdriver.Firefox(), etc.\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://www.boerse-frankfurt.de/anleihen/most-traded\"\n",
        "\n",
        "# Function to extract bond data from the current page\n",
        "def extract_bond_data(soup):\n",
        "    bond_table = soup.find(\"div\", {\"class\": \"table-responsive\"})\n",
        "    bonds = []\n",
        "\n",
        "    if bond_table:\n",
        "        rows = bond_table.find_all(\"tr\")  # Get all rows in the table\n",
        "\n",
        "        # Iterate over each row (skip the header row)\n",
        "        for row in rows[1:]:\n",
        "            cols = row.find_all(\"td\")  # Get all columns in the row\n",
        "            if len(cols) >= 6:  # Ensure there are enough columns\n",
        "                bond_info = {\n",
        "                    \"Name\": cols[0].text.strip(),\n",
        "                    \"WKN\": cols[1].text.strip(),\n",
        "                    \"Letzter Preis\": cols[2].text.strip(),\n",
        "                    \"Datum/Zeit letzter Preis\": cols[3].text.strip(),\n",
        "                    \"Umsatz in Euro\": cols[4].text.strip(),\n",
        "                    \"+/- %\": cols[5].text.strip(),\n",
        "                    \"Coupon\": cols[12].text.strip(),\n",
        "                    \"Currency\": cols[13].text.strip(),\n",
        "                    \"YTM\": cols[14].text.strip()\n",
        "                }\n",
        "                bonds.append(bond_info)\n",
        "    return bonds\n",
        "\n",
        "try:\n",
        "    # Open the website\n",
        "    driver.get(url)\n",
        "\n",
        "    # Check if needed\n",
        "    time.sleep(5)\n",
        "\n",
        "    # <button _ngcontent-boerse-frankfurt-c115\n",
        "    # Wait for the \"100\" button to be visible and clickable\n",
        "    wait = WebDriverWait(driver, 1)\n",
        "    hundred_button = wait.until(\n",
        "        EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'page-bar-type-button btn btn-lg ng-star-inserted') and text()='100']\"))\n",
        "    )\n",
        "    # Click the \"100\" button\n",
        "    hundred_button.click()\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Initialize an empty list to store all bonds\n",
        "    all_bonds = []\n",
        "\n",
        "    # Find the number of pages\n",
        "    page_buttons = driver.find_elements(By.XPATH, \"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and not(@disabled)]\")\n",
        "    total_pages = int(page_buttons[-1].text.strip())  # Extract the number from the last button, this button should always be showing the maximum amount of pages\n",
        "    print(f\"Total pages: {total_pages}\")\n",
        "\n",
        "    # Loop through all pages, skipping the first one, limiting to 20 pages, so the 2000 most traded bonds\n",
        "    for page in range(1, max(total_pages + 1, 21)):\n",
        "\n",
        "        # Wait for the page button to be clickable and click it\n",
        "        if page != 1:\n",
        "          page_button = wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH, f\"//button[contains(@class, 'page-bar-type-button page-bar-type-button-width-auto btn btn-lg ng-star-inserted') and text()='{page}']\"))\n",
        "          )\n",
        "          page_button.click()\n",
        "          time.sleep(5)  # Allow time for the page to load, check how long is optimal\n",
        "\n",
        "        # Extract page source and parse with BeautifulSoup\n",
        "        page_source = driver.page_source\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "        # Extract data from the current page\n",
        "        bonds = extract_bond_data(soup)\n",
        "        all_bonds.extend(bonds)\n",
        "\n",
        "        # Loop count\n",
        "        print(f\"Loop {page}\")\n",
        "\n",
        "    # Save all the extracted data to a CSV file\n",
        "    with open(\"all_bonds_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"Name\", \"WKN\", \"Letzter Preis\", \"Datum/Zeit letzter Preis\", \"Umsatz in Euro\", \"+/- %\", \"Coupon\", \"Currency\", \"YTM\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()  # Write the header row\n",
        "        writer.writerows(all_bonds)  # Write all bond data rows\n",
        "\n",
        "    print(\"Bond data has been saved to 'all_bonds_data.csv'\")\n",
        "\n",
        "finally:\n",
        "    # Close the browser\n",
        "    driver.quit()"
      ]
    }
  ]
}